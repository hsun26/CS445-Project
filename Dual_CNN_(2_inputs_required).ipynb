{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOk5KhD4cPQWdAGkoJwyIuO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsun26/CS445-Project/blob/main/Dual_CNN_(2_inputs_required).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cynCksD2SiUf",
        "outputId": "ad42b0d8-2a07-416c-e6d4-d0c04929b533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmFoXi0SH9wE",
        "outputId": "b7c41a31-5c40-4094-ba0d-9bf70cfa62e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "tGzqx2GnICIK"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "\n",
        "class HandGestureDataset(Dataset):\n",
        "    def __init__(self, hand_dir, mask_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hand_dir (string): Directory with all the hand images divided into subdirectories.\n",
        "            mask_dir (string): Directory with all the mask images divided into the same subdirectories as hand images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.hand_dir = hand_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.samples = self._load_samples()\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        # Labels could be dynamically assigned based on folder names if needed\n",
        "        label_mapping = {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, 'like': 5, 'ok': 6}\n",
        "\n",
        "        categories = os.listdir(self.hand_dir)\n",
        "        for category in categories:\n",
        "            hand_path = os.path.join(self.hand_dir, category)\n",
        "            mask_path = os.path.join(self.mask_dir, category)\n",
        "            for filename in os.listdir(hand_path):\n",
        "                if filename.endswith('.jpg'):  # Make sure to match your image file extensions\n",
        "                    file_hand_path = os.path.join(hand_path, filename)\n",
        "                    file_mask_path = os.path.join(mask_path, filename.replace('.jpg', '.bmp'))\n",
        "                    samples.append((file_hand_path, file_mask_path, label_mapping[category]))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hand_path, mask_path, label = self.samples[idx]\n",
        "\n",
        "        hand_image = Image.open(hand_path).convert('RGB')\n",
        "        mask_image = Image.open(mask_path).convert('L')  # Ensure mask is in grayscale\n",
        "\n",
        "        if self.transform:\n",
        "            hand_image = self.transform(hand_image)\n",
        "            mask_image = self.transform(mask_image)\n",
        "\n",
        "        return hand_image, mask_image, label\n",
        "\n",
        "# Transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ],
      "metadata": {
        "id": "Q_sRC9kX41Ng"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hand_dir = '/content/drive/MyDrive/CS445/Final Project/filtered_hands (1)'\n",
        "mask_dir = '/content/drive/MyDrive/CS445/Final Project/filtered_masks (1)'\n",
        "dataset = HandGestureDataset(hand_dir, mask_dir, transform=transform)\n",
        "\n",
        "# Split the dataset into training and testing\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# DataLoader setup\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n"
      ],
      "metadata": {
        "id": "FGHeySMBrQpB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DC-CNN (pytorch)"
      ],
      "metadata": {
        "id": "hwcD-kt4mBWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DualCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DualCNN, self).__init__()\n",
        "        # Define the first branch for the RGB image\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 20, kernel_size=5, padding=2),  # Padding=2 to keep size constant\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),  # Larger kernel and padding\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Define the second branch for the mask image\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(1, 20, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Define the fully connected layers\n",
        "        self.fc1 = nn.Linear(20 * 56 * 56 * 2, 224)  # calculated\n",
        "        self.fc2 = nn.Linear(224, 7)\n",
        "\n",
        "    def forward(self, x_img, x_mask):\n",
        "        out_img = self.branch1(x_img)\n",
        "        out_mask = self.branch2(x_mask)\n",
        "\n",
        "        # print(\"Output size after branch1:\", out_img.shape)  # Debug: Check output size\n",
        "        # print(\"Output size after branch2:\", out_mask.shape)\n",
        "\n",
        "        out_img = out_img.view(out_img.size(0), -1)\n",
        "        out_mask = out_mask.view(out_mask.size(0), -1)\n",
        "        out = torch.cat((out_img, out_mask), dim=1)\n",
        "\n",
        "        # print(\"Concatenated output size:\", out.shape)  # Debug: Check concatenated size\n",
        "\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Initialize model\n",
        "model = DualCNN()\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzGiqPmAHqA2",
        "outputId": "b4e772d1-bc3a-4522-ce78-2e89c35d4737"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DualCNN(\n",
            "  (branch1): Sequential(\n",
            "    (0): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(20, 20, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (branch2): Sequential(\n",
            "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(20, 20, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Linear(in_features=125440, out_features=224, bias=True)\n",
            "  (fc2): Linear(in_features=224, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DualCNN()\n",
        "summary(model, [(3, 224, 224), (1, 224, 224)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgXo8D6wHrpV",
        "outputId": "3618c9f0-306d-4be6-ae14-72422a73639b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 20, 224, 224]           1,520\n",
            "              ReLU-2         [-1, 20, 224, 224]               0\n",
            "         MaxPool2d-3         [-1, 20, 112, 112]               0\n",
            "            Conv2d-4         [-1, 20, 112, 112]          19,620\n",
            "              ReLU-5         [-1, 20, 112, 112]               0\n",
            "         MaxPool2d-6           [-1, 20, 56, 56]               0\n",
            "            Conv2d-7         [-1, 20, 224, 224]             520\n",
            "              ReLU-8         [-1, 20, 224, 224]               0\n",
            "         MaxPool2d-9         [-1, 20, 112, 112]               0\n",
            "           Conv2d-10         [-1, 20, 112, 112]          19,620\n",
            "             ReLU-11         [-1, 20, 112, 112]               0\n",
            "        MaxPool2d-12           [-1, 20, 56, 56]               0\n",
            "           Linear-13                  [-1, 224]      28,098,784\n",
            "           Linear-14                    [-1, 7]           1,575\n",
            "================================================================\n",
            "Total params: 28,141,639\n",
            "Trainable params: 28,141,639\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 28812.00\n",
            "Forward/backward pass size (MB): 43.07\n",
            "Params size (MB): 107.35\n",
            "Estimated Total Size (MB): 28962.42\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def train(model, criterion, optimizer, train_loader, epochs=10):\n",
        "#     model.train()\n",
        "#     for epoch in range(epochs):\n",
        "#         for data_img, data_mask, labels in train_loader:\n",
        "#             optimizer.zero_grad()   # 清除过去的梯度\n",
        "#             outputs = model(data_img, data_mask)   # get output\n",
        "#             loss = criterion(outputs, labels)   # calculate loss\n",
        "#             loss.backward()    # backpropogation\n",
        "#             optimizer.step()   # 更新模型的参数\n",
        "#         print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "uK1LpeMZHtUI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluate(model, test_loader):\n",
        "#     model.eval()\n",
        "#     total = 0\n",
        "#     correct = 0\n",
        "#     with torch.no_grad():\n",
        "#         for data_img, data_mask, labels in test_loader:\n",
        "#             outputs = model(data_img, data_mask)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "#     accuracy = 100 * correct / total\n",
        "#     return accuracy\n"
      ],
      "metadata": {
        "id": "B8eR_VTazFcp"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_and_evaluate(model, criterion, optimizer, train_loader, test_loader, epochs=10):\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         train_loss = 0\n",
        "#         total_batches = len(train_loader)\n",
        "#         loop = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=True)\n",
        "\n",
        "#         for data_img, data_mask, labels in loop:\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(data_img, data_mask)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             train_loss += loss.item()\n",
        "#             loop.set_postfix(loss=loss.item())\n",
        "\n",
        "#         train_loss /= total_batches\n",
        "#         validation_accuracy = evaluate(model, test_loader)\n",
        "\n",
        "#         # Print training and validation results\n",
        "#         print(f'Epoch {epoch + 1}: Training Loss: {train_loss:.4f}, Validation Accuracy: {validation_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "gOncBVStx-Ez"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(model, criterion, optimizer, train_loader, val_loader, n_epochs=10):\n",
        "    for epoch in range(n_epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_counter = 0\n",
        "\n",
        "        train_tqdm = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
        "        for data_img, data_mask, labels in train_tqdm:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data_img, data_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate loss and accuracy\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            train_counter += labels.size(0)\n",
        "\n",
        "            train_tqdm.set_postfix(loss=train_loss/(1+len(train_tqdm)), accuracy=100.0 * train_correct / train_counter)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_counter = 0\n",
        "\n",
        "        val_tqdm = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}')\n",
        "        with torch.no_grad():\n",
        "            for data_img, data_mask, labels in val_tqdm:\n",
        "                outputs = model(data_img, data_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_counter += labels.size(0)\n",
        "\n",
        "                val_tqdm.set_postfix(loss=val_loss/(1+len(val_tqdm)), accuracy=100.0 * val_correct / val_counter)\n",
        "\n",
        "        # End of Epoch Summary\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy = 100.0 * train_correct / train_counter\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = 100.0 * val_correct / val_counter\n",
        "        print(f\"Epoch {epoch+1}: Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
        "        print(f\"Epoch {epoch+1}: Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "578LwcBN2j9E"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DualCNN()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)   # update the model's parameters (weights and biases) during the training\n",
        "criterion = nn.CrossEntropyLoss()  #define loss function"
      ],
      "metadata": {
        "id": "Z9pT6Wdzzevy"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate(model, criterion, optimizer, train_loader, test_loader, n_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDbUs-Wi2nXh",
        "outputId": "aec5e38e-949f-4aa3-aa41-a98effd18f59"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 27/27 [00:44<00:00,  1.65s/it, accuracy=29.9, loss=1.69]\n",
            "Validation Epoch 1: 100%|██████████| 7/7 [00:04<00:00,  1.49it/s, accuracy=47, loss=1.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Training Loss: 1.7496, Training Accuracy: 29.89%\n",
            "Epoch 1: Validation Loss: 1.3312, Validation Accuracy: 46.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 27/27 [00:43<00:00,  1.61s/it, accuracy=44.4, loss=1.3]\n",
            "Validation Epoch 2: 100%|██████████| 7/7 [00:04<00:00,  1.74it/s, accuracy=28.8, loss=1.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Training Loss: 1.3512, Training Accuracy: 44.44%\n",
            "Epoch 2: Validation Loss: 1.8239, Validation Accuracy: 28.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 27/27 [00:41<00:00,  1.55s/it, accuracy=41, loss=1.66]\n",
            "Validation Epoch 3: 100%|██████████| 7/7 [00:05<00:00,  1.33it/s, accuracy=39.4, loss=1.44]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Training Loss: 1.7249, Training Accuracy: 41.00%\n",
            "Epoch 3: Validation Loss: 1.6407, Validation Accuracy: 39.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 27/27 [00:42<00:00,  1.57s/it, accuracy=47.1, loss=1.29]\n",
            "Validation Epoch 4: 100%|██████████| 7/7 [00:04<00:00,  1.74it/s, accuracy=36.4, loss=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Training Loss: 1.3411, Training Accuracy: 47.13%\n",
            "Epoch 4: Validation Loss: 1.3844, Validation Accuracy: 36.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 27/27 [00:45<00:00,  1.67s/it, accuracy=62.5, loss=1.1]\n",
            "Validation Epoch 5: 100%|██████████| 7/7 [00:04<00:00,  1.51it/s, accuracy=53, loss=0.97]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Training Loss: 1.1414, Training Accuracy: 62.45%\n",
            "Epoch 5: Validation Loss: 1.1087, Validation Accuracy: 53.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 27/27 [00:43<00:00,  1.60s/it, accuracy=75.1, loss=0.659]\n",
            "Validation Epoch 6: 100%|██████████| 7/7 [00:04<00:00,  1.74it/s, accuracy=48.5, loss=1.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Training Loss: 0.6835, Training Accuracy: 75.10%\n",
            "Epoch 6: Validation Loss: 1.5985, Validation Accuracy: 48.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 27/27 [00:42<00:00,  1.58s/it, accuracy=73.9, loss=0.647]\n",
            "Validation Epoch 7: 100%|██████████| 7/7 [00:04<00:00,  1.74it/s, accuracy=54.5, loss=1.07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Training Loss: 0.6712, Training Accuracy: 73.95%\n",
            "Epoch 7: Validation Loss: 1.2189, Validation Accuracy: 54.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 27/27 [00:42<00:00,  1.57s/it, accuracy=90.4, loss=0.298]\n",
            "Validation Epoch 8: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s, accuracy=59.1, loss=0.998]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Training Loss: 0.3086, Training Accuracy: 90.42%\n",
            "Epoch 8: Validation Loss: 1.1405, Validation Accuracy: 59.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 27/27 [00:45<00:00,  1.67s/it, accuracy=96.2, loss=0.0996]\n",
            "Validation Epoch 9: 100%|██████████| 7/7 [00:03<00:00,  1.76it/s, accuracy=63.6, loss=1.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Training Loss: 0.1032, Training Accuracy: 96.17%\n",
            "Epoch 9: Validation Loss: 1.3145, Validation Accuracy: 63.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 27/27 [00:42<00:00,  1.56s/it, accuracy=100, loss=0.0109]\n",
            "Validation Epoch 10: 100%|██████████| 7/7 [00:04<00:00,  1.54it/s, accuracy=62.1, loss=1.32]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Training Loss: 0.0113, Training Accuracy: 100.00%\n",
            "Epoch 10: Validation Loss: 1.5093, Validation Accuracy: 62.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "\n",
        "def load_image(image_path, mask_path=None, image_size=224):\n",
        "    # Image transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        # Normalize with the same parameters used in training\n",
        "\n",
        "    ])\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image)\n",
        "\n",
        "    # If a mask is required\n",
        "    if mask_path:\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        mask = transform(mask)\n",
        "    else:\n",
        "        mask = None\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "def test_model(model, image_path, mask_path=None):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        image, mask = load_image(image_path, mask_path)\n",
        "        if mask is not None:\n",
        "            image, mask = image.unsqueeze(0), mask.unsqueeze(0)  # Add batch dimension\n",
        "            outputs = model(image, mask)\n",
        "        else:\n",
        "            image = image.unsqueeze(0)  # Add batch dimension\n",
        "            outputs = model(image)\n",
        "\n",
        "        # Assuming the output is class scores\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        return predicted.item()\n",
        "\n",
        "\n",
        "model = DualCNN()\n",
        "\n",
        "predicted_class = test_model(model, '/content/drive/MyDrive/CS445/Final Project/1_A_hgr2A1_id02_1.jpg', '/content/drive/MyDrive/CS445/Final Project/1_A_hgr2A1_id02_1.bmp')\n",
        "print(\"Predicted Class:\", predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVJRekMz7YEB",
        "outputId": "d935e67c-d575-4120-e002-e5a6537965df"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class = test_model(model, 'path_to_test_image.jpg', 'path_to_test_mask.bmp')\n",
        "print(\"Predicted Class:\", predicted_class)"
      ],
      "metadata": {
        "id": "72lL2qPX7JUY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}