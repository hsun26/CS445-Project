{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsun26/CS445-Project/blob/main/Dual_CNN_(use_generated_mask).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cynCksD2SiUf",
        "outputId": "da83918c-3e45-49c4-cb46-6b20eaafd15f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmFoXi0SH9wE",
        "outputId": "d9fd308b-1f9a-4655-e8c8-8e451f3d6a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0UQvbTBISIT",
        "outputId": "53c213b5-9dbb-4d76-89df-b5fe25b4ad6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.3 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGzqx2GnICIK"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZauK8Y5AckVx"
      },
      "outputs": [],
      "source": [
        "class HandGestureDataset(Dataset):\n",
        "    def __init__(self, rgb_dir, mask_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            rgb_dir (string): Directory with all the RGB images divided into subdirectories.\n",
        "            mask_dir (string): Directory with all the mask images divided into the same subdirectories as RGB images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.rgb_dir = rgb_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.samples = self._load_samples()\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        label_mapping = {'one': 0, 'two': 1, 'three': 2, 'four': 3, 'five': 4, 'like': 5, 'ok': 6}\n",
        "\n",
        "        # Iterate over all categories\n",
        "        for category in label_mapping:\n",
        "            rgb_path = os.path.join(self.rgb_dir, category)\n",
        "            mask_path = os.path.join(self.mask_dir, category)\n",
        "            for filename in os.listdir(rgb_path):\n",
        "                if filename.endswith('.jpg'):\n",
        "                    # Constructing the mask filename based on the RGB filename\n",
        "                    # RGB filename like '2_Original_046.jpg' corresponds to mask '2_Mask_046.jpg'\n",
        "                    mask_filename = filename.replace('Original', 'Mask')   # so here I\n",
        "                    file_rgb_path = os.path.join(rgb_path, filename)\n",
        "                    file_mask_path = os.path.join(mask_path, mask_filename)\n",
        "                    samples.append((file_rgb_path, file_mask_path, label_mapping[category]))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rgb_path, mask_path, label = self.samples[idx]\n",
        "        rgb_image = Image.open(rgb_path).convert('RGB')\n",
        "        mask_image = Image.open(mask_path).convert('L')  # Assuming masks are grayscale\n",
        "\n",
        "        if self.transform:\n",
        "            rgb_image = self.transform(rgb_image)\n",
        "            mask_image = self.transform(mask_image)\n",
        "\n",
        "        return rgb_image, mask_image, label\n",
        "\n",
        "# Transformation to apply\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu4Jh5wkejIk",
        "outputId": "56c57e96-ebc9-4a8e-d48a-9053e58e802d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('/content/drive/MyDrive/CS445/Final Project/rgb/one/1_Original_018.jpg', '/content/drive/MyDrive/CS445/Final Project/generate/one/1_Mask_018.jpg', 0)\n",
            "('/content/drive/MyDrive/CS445/Final Project/rgb/one/1_Original_022.jpg', '/content/drive/MyDrive/CS445/Final Project/generate/one/1_Mask_022.jpg', 0)\n",
            "('/content/drive/MyDrive/CS445/Final Project/rgb/one/1_Original_010.jpg', '/content/drive/MyDrive/CS445/Final Project/generate/one/1_Mask_010.jpg', 0)\n",
            "('/content/drive/MyDrive/CS445/Final Project/rgb/one/1_Original_021.jpg', '/content/drive/MyDrive/CS445/Final Project/generate/one/1_Mask_021.jpg', 0)\n",
            "('/content/drive/MyDrive/CS445/Final Project/rgb/one/1_Original_024.jpg', '/content/drive/MyDrive/CS445/Final Project/generate/one/1_Mask_024.jpg', 0)\n"
          ]
        }
      ],
      "source": [
        "dataset = HandGestureDataset('/content/drive/MyDrive/CS445/Final Project/rgb', '/content/drive/MyDrive/CS445/Final Project/generate', transform=transform)\n",
        "for i in range(5):  # Check first 5 samples\n",
        "    print(dataset.samples[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGHeySMBrQpB"
      },
      "outputs": [],
      "source": [
        "# hand_dir = '/content/drive/MyDrive/CS445/Final Project/rgb'\n",
        "# mask_dir = '/content/drive/MyDrive/CS445/Final Project/generate'\n",
        "# dataset = HandGestureDataset(hand_dir, mask_dir, transform=transform)\n",
        "\n",
        "# Split the dataset into training and testing\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# DataLoader setup\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwcD-kt4mBWD"
      },
      "source": [
        "### DC-CNN (pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzGiqPmAHqA2",
        "outputId": "7afbf2c8-1d83-4ac1-ced9-84663025f64b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DualCNN(\n",
            "  (branch1): Sequential(\n",
            "    (0): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(20, 20, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (branch2): Sequential(\n",
            "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(20, 20, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Linear(in_features=125440, out_features=224, bias=True)\n",
            "  (fc2): Linear(in_features=224, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class DualCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DualCNN, self).__init__()\n",
        "        # Define the first branch for the RGB image\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 20, kernel_size=5, padding=2),  # Padding=2 to keep size constant\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),  # Larger kernel and padding\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Define the second branch for the mask image\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(1, 20, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Define the fully connected layers\n",
        "        self.fc1 = nn.Linear(20 * 56 * 56 * 2, 224)  # calculated\n",
        "        self.fc2 = nn.Linear(224, 7)\n",
        "\n",
        "    def forward(self, x_img, x_mask):\n",
        "        out_img = self.branch1(x_img)\n",
        "        out_mask = self.branch2(x_mask)\n",
        "\n",
        "        # print(\"Output size after branch1:\", out_img.shape)  # Debug: Check output size\n",
        "        # print(\"Output size after branch2:\", out_mask.shape)\n",
        "\n",
        "        out_img = out_img.view(out_img.size(0), -1)\n",
        "        out_mask = out_mask.view(out_mask.size(0), -1)\n",
        "        out = torch.cat((out_img, out_mask), dim=1)\n",
        "\n",
        "        # print(\"Concatenated output size:\", out.shape)  # Debug: Check concatenated size\n",
        "\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Initialize model\n",
        "model = DualCNN()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgXo8D6wHrpV",
        "outputId": "c7b45769-ca67-4136-82d1-6b8840c96db1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 20, 224, 224]           1,520\n",
            "              ReLU-2         [-1, 20, 224, 224]               0\n",
            "         MaxPool2d-3         [-1, 20, 112, 112]               0\n",
            "            Conv2d-4         [-1, 20, 112, 112]          19,620\n",
            "              ReLU-5         [-1, 20, 112, 112]               0\n",
            "         MaxPool2d-6           [-1, 20, 56, 56]               0\n",
            "            Conv2d-7         [-1, 20, 224, 224]             520\n",
            "              ReLU-8         [-1, 20, 224, 224]               0\n",
            "         MaxPool2d-9         [-1, 20, 112, 112]               0\n",
            "           Conv2d-10         [-1, 20, 112, 112]          19,620\n",
            "             ReLU-11         [-1, 20, 112, 112]               0\n",
            "        MaxPool2d-12           [-1, 20, 56, 56]               0\n",
            "           Linear-13                  [-1, 224]      28,098,784\n",
            "           Linear-14                    [-1, 7]           1,575\n",
            "================================================================\n",
            "Total params: 28,141,639\n",
            "Trainable params: 28,141,639\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 28812.00\n",
            "Forward/backward pass size (MB): 43.07\n",
            "Params size (MB): 107.35\n",
            "Estimated Total Size (MB): 28962.42\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = DualCNN()\n",
        "summary(model, [(3, 224, 224), (1, 224, 224)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "578LwcBN2j9E"
      },
      "outputs": [],
      "source": [
        "def train_and_validate(model, criterion, optimizer, train_loader, val_loader, n_epochs=10):\n",
        "    for epoch in range(n_epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_counter = 0\n",
        "\n",
        "        train_tqdm = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
        "        for data_img, data_mask, labels in train_tqdm:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data_img, data_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate loss and accuracy\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            train_counter += labels.size(0)\n",
        "\n",
        "            train_tqdm.set_postfix(loss=train_loss/(1+len(train_tqdm)), accuracy=100.0 * train_correct / train_counter)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_counter = 0\n",
        "\n",
        "        val_tqdm = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}')\n",
        "        with torch.no_grad():\n",
        "            for data_img, data_mask, labels in val_tqdm:\n",
        "                outputs = model(data_img, data_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_counter += labels.size(0)\n",
        "\n",
        "                val_tqdm.set_postfix(loss=val_loss/(1+len(val_tqdm)), accuracy=100.0 * val_correct / val_counter)\n",
        "\n",
        "        # End of Epoch Summary\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy = 100.0 * train_correct / train_counter\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = 100.0 * val_correct / val_counter\n",
        "        print(f\"Epoch {epoch+1}: Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
        "        print(f\"Epoch {epoch+1}: Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpTmFYrm9ebz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_validate(model, criterion, optimizer, train_loader, val_loader, n_epochs=10):\n",
        "    for epoch in range(n_epochs):\n",
        "        # Initialize metrics\n",
        "        train_losses, val_losses = [], []\n",
        "        train_preds, train_targets = [], []\n",
        "        val_preds, val_targets = [], []\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_tqdm = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
        "        for data_img, data_mask, labels in train_tqdm:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data_img, data_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss and predictions\n",
        "            train_losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_preds.extend(predicted.cpu().numpy())\n",
        "            train_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            train_tqdm.set_postfix(loss=np.mean(train_losses))\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_tqdm = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}')\n",
        "        with torch.no_grad():\n",
        "            for data_img, data_mask, labels in val_tqdm:\n",
        "                outputs = model(data_img, data_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Accumulate loss and predictions\n",
        "                val_losses.append(loss.item())\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_preds.extend(predicted.cpu().numpy())\n",
        "                val_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                val_tqdm.set_postfix(loss=np.mean(val_losses))\n",
        "\n",
        "        # Calculate metrics for training\n",
        "        train_precision = precision_score(train_targets, train_preds, average='macro')\n",
        "        train_recall = recall_score(train_targets, train_preds, average='macro')\n",
        "        train_f1 = f1_score(train_targets, train_preds, average='macro')\n",
        "\n",
        "        # Calculate metrics for validation\n",
        "        val_precision = precision_score(val_targets, val_preds, average='macro')\n",
        "        val_recall = recall_score(val_targets, val_preds, average='macro')\n",
        "        val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
        "\n",
        "        # End of Epoch Summary\n",
        "        print(f'Epoch {epoch+1}: Training Loss: {np.mean(train_losses):.4f}')\n",
        "        print(f'Training Precision: {train_precision:.2f}, Recall: {train_recall:.2f}, F1: {train_f1:.2f}')\n",
        "        print(f'Validation Loss: {np.mean(val_losses):.4f}')\n",
        "        print(f'Validation Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1: {val_f1:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9pT6Wdzzevy"
      },
      "outputs": [],
      "source": [
        "model = DualCNN()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)   # update the model's parameters (weights and biases) during the training\n",
        "criterion = nn.CrossEntropyLoss()  #define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0jwuziq-v5A",
        "outputId": "20e053cd-27f8-43b0-c19b-44ad76de72e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|██████████| 21/21 [03:59<00:00, 11.42s/it, loss=1.91]\n",
            "Validation Epoch 1: 100%|██████████| 6/6 [00:52<00:00,  8.69s/it, loss=1.84]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Training Loss: 1.9150\n",
            "Training Precision: 0.12, Recall: 0.16, F1: 0.11\n",
            "Validation Loss: 1.8355\n",
            "Validation Precision: 0.06, Recall: 0.14, F1: 0.08\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|██████████| 21/21 [01:31<00:00,  4.34s/it, loss=1.82]\n",
            "Validation Epoch 2: 100%|██████████| 6/6 [00:18<00:00,  3.14s/it, loss=1.76]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Training Loss: 1.8194\n",
            "Training Precision: 0.37, Recall: 0.28, F1: 0.29\n",
            "Validation Loss: 1.7610\n",
            "Validation Precision: 0.08, Recall: 0.18, F1: 0.11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|██████████| 21/21 [01:29<00:00,  4.26s/it, loss=1.62]\n",
            "Validation Epoch 3: 100%|██████████| 6/6 [00:24<00:00,  4.10s/it, loss=1.69]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Training Loss: 1.6242\n",
            "Training Precision: 0.34, Recall: 0.30, F1: 0.30\n",
            "Validation Loss: 1.6890\n",
            "Validation Precision: 0.21, Recall: 0.28, F1: 0.23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|██████████| 21/21 [01:28<00:00,  4.22s/it, loss=1.44]\n",
            "Validation Epoch 4: 100%|██████████| 6/6 [00:16<00:00,  2.76s/it, loss=1.77]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Training Loss: 1.4388\n",
            "Training Precision: 0.49, Recall: 0.43, F1: 0.44\n",
            "Validation Loss: 1.7654\n",
            "Validation Precision: 0.34, Recall: 0.35, F1: 0.28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|██████████| 21/21 [01:37<00:00,  4.64s/it, loss=1.52]\n",
            "Validation Epoch 5: 100%|██████████| 6/6 [00:15<00:00,  2.58s/it, loss=1.92]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Training Loss: 1.5162\n",
            "Training Precision: 0.46, Recall: 0.39, F1: 0.39\n",
            "Validation Loss: 1.9203\n",
            "Validation Precision: 0.35, Recall: 0.44, F1: 0.33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|██████████| 21/21 [01:43<00:00,  4.94s/it, loss=1.23]\n",
            "Validation Epoch 6: 100%|██████████| 6/6 [00:16<00:00,  2.70s/it, loss=1.65]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Training Loss: 1.2251\n",
            "Training Precision: 0.54, Recall: 0.51, F1: 0.51\n",
            "Validation Loss: 1.6498\n",
            "Validation Precision: 0.36, Recall: 0.44, F1: 0.36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|██████████| 21/21 [01:30<00:00,  4.32s/it, loss=1.01]\n",
            "Validation Epoch 7: 100%|██████████| 6/6 [00:17<00:00,  2.95s/it, loss=1.83]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Training Loss: 1.0111\n",
            "Training Precision: 0.68, Recall: 0.62, F1: 0.64\n",
            "Validation Loss: 1.8263\n",
            "Validation Precision: 0.45, Recall: 0.45, F1: 0.35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|██████████| 21/21 [01:33<00:00,  4.44s/it, loss=0.703]\n",
            "Validation Epoch 8: 100%|██████████| 6/6 [00:16<00:00,  2.74s/it, loss=1.92]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Training Loss: 0.7032\n",
            "Training Precision: 0.74, Recall: 0.73, F1: 0.73\n",
            "Validation Loss: 1.9244\n",
            "Validation Precision: 0.34, Recall: 0.37, F1: 0.34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|██████████| 21/21 [01:30<00:00,  4.30s/it, loss=0.57]\n",
            "Validation Epoch 9: 100%|██████████| 6/6 [00:13<00:00,  2.22s/it, loss=1.97]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Training Loss: 0.5699\n",
            "Training Precision: 0.82, Recall: 0.81, F1: 0.81\n",
            "Validation Loss: 1.9652\n",
            "Validation Precision: 0.28, Recall: 0.31, F1: 0.27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|██████████| 21/21 [01:31<00:00,  4.35s/it, loss=0.526]\n",
            "Validation Epoch 10: 100%|██████████| 6/6 [00:16<00:00,  2.68s/it, loss=1.92]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Training Loss: 0.5261\n",
            "Training Precision: 0.86, Recall: 0.87, F1: 0.86\n",
            "Validation Loss: 1.9196\n",
            "Validation Precision: 0.43, Recall: 0.40, F1: 0.39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 原始设置\n",
        "train_and_validate(model, criterion, optimizer, train_loader, test_loader, n_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIKsVqCkH1LV"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVJRekMz7YEB",
        "outputId": "d935e67c-d575-4120-e002-e5a6537965df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Class: 0\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "\n",
        "def load_image(image_path, mask_path=None, image_size=224):\n",
        "    # Image transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        # Normalize with the same parameters used in training\n",
        "\n",
        "    ])\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image)\n",
        "\n",
        "    # If a mask is required\n",
        "    if mask_path:\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        mask = transform(mask)\n",
        "    else:\n",
        "        mask = None\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "def test_model(model, image_path, mask_path=None):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        image, mask = load_image(image_path, mask_path)\n",
        "        if mask is not None:\n",
        "            image, mask = image.unsqueeze(0), mask.unsqueeze(0)  # Add batch dimension\n",
        "            outputs = model(image, mask)\n",
        "        else:\n",
        "            image = image.unsqueeze(0)  # Add batch dimension\n",
        "            outputs = model(image)\n",
        "\n",
        "        # the output is class scores\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        return predicted.item()\n",
        "\n",
        "\n",
        "model = DualCNN()\n",
        "\n",
        "predicted_class = test_model(model, '/content/drive/MyDrive/CS445/Final Project/1_A_hgr2A1_id02_1.jpg', '/content/drive/MyDrive/CS445/Final Project/1_A_hgr2A1_id02_1.bmp')\n",
        "print(\"Predicted Class:\", predicted_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72lL2qPX7JUY"
      },
      "outputs": [],
      "source": [
        "predicted_class = test_model(model, 'path_to_test_image.jpg', 'path_to_test_mask.bmp')\n",
        "print(\"Predicted Class:\", predicted_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7h4bEzS1Qv1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwbLeozn_aJu"
      },
      "source": [
        "### DCCNN (use different regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5UViq4V_X3S",
        "outputId": "89076bf2-d392-4ee8-fae1-32eb6696ca4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DualCNN(\n",
            "  (branch1): Sequential(\n",
            "    (0): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(20, 20, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (branch2): Sequential(\n",
            "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(20, 20, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (fc1): Linear(in_features=125440, out_features=224, bias=True)\n",
            "  (fc2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=224, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DualCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DualCNN, self).__init__()\n",
        "        # Define the first branch for the RGB image\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 20, kernel_size=5, padding=2),  # Padding=2 to keep size constant\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),  # Larger kernel and padding\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.5)  # Dropout added after pooling\n",
        "        )\n",
        "\n",
        "        # Define the second branch for the mask image\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(1, 20, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.5)  # Dropout added after pooling\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(20 * 56 * 56 * 2, 224)\n",
        "        self.fc2 = nn.Dropout(0.5)  # Dropout before the final classification layer\n",
        "        self.fc3 = nn.Linear(224, 7)\n",
        "\n",
        "    def forward(self, x_img, x_mask):\n",
        "        out_img = self.branch1(x_img)\n",
        "        out_mask = self.branch2(x_mask)\n",
        "\n",
        "        out_img = out_img.view(out_img.size(0), -1)\n",
        "        out_mask = out_mask.view(out_mask.size(0), -1)\n",
        "        out = torch.cat((out_img, out_mask), dim=1)\n",
        "\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)  # Applying dropout\n",
        "        out = self.fc3(out)  # Final output layer\n",
        "        return out\n",
        "\n",
        "# Initialize model\n",
        "model = DualCNN()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePw6NULL_X3f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_validate(model, criterion, optimizer, train_loader, val_loader, n_epochs=10):\n",
        "    for epoch in range(n_epochs):\n",
        "        # Initialize metrics\n",
        "        train_losses, val_losses = [], []\n",
        "        train_preds, train_targets = [], []\n",
        "        val_preds, val_targets = [], []\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_tqdm = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
        "        for data_img, data_mask, labels in train_tqdm:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data_img, data_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss and predictions\n",
        "            train_losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_preds.extend(predicted.cpu().numpy())\n",
        "            train_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            train_tqdm.set_postfix(loss=np.mean(train_losses))\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_tqdm = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}')\n",
        "        with torch.no_grad():\n",
        "            for data_img, data_mask, labels in val_tqdm:\n",
        "                outputs = model(data_img, data_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Accumulate loss and predictions\n",
        "                val_losses.append(loss.item())\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_preds.extend(predicted.cpu().numpy())\n",
        "                val_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                val_tqdm.set_postfix(loss=np.mean(val_losses))\n",
        "\n",
        "        # Calculate metrics for training\n",
        "        train_precision = precision_score(train_targets, train_preds, average='macro')\n",
        "        train_recall = recall_score(train_targets, train_preds, average='macro')\n",
        "        train_f1 = f1_score(train_targets, train_preds, average='macro')\n",
        "\n",
        "        # Calculate metrics for validation\n",
        "        val_precision = precision_score(val_targets, val_preds, average='macro')\n",
        "        val_recall = recall_score(val_targets, val_preds, average='macro')\n",
        "        val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
        "\n",
        "        # End of Epoch Summary\n",
        "        print(f'Epoch {epoch+1}: Training Loss: {np.mean(train_losses):.4f}')\n",
        "        print(f'Training Precision: {train_precision:.2f}, Recall: {train_recall:.2f}, F1: {train_f1:.2f}')\n",
        "        print(f'Validation Loss: {np.mean(val_losses):.4f}')\n",
        "        print(f'Validation Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1: {val_f1:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olV0sq6i_X3g"
      },
      "outputs": [],
      "source": [
        "model = DualCNN()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)   # update the model's parameters (weights and biases) during the training\n",
        "criterion = nn.CrossEntropyLoss()  #define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWQS4Wca_X3g",
        "outputId": "79540c81-f7f5-43b8-bfb5-5302d8a1ce9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|██████████| 21/21 [01:27<00:00,  4.18s/it, loss=1.9]\n",
            "Validation Epoch 1: 100%|██████████| 6/6 [00:13<00:00,  2.30s/it, loss=1.86]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Training Loss: 1.9040\n",
            "Training Precision: 0.17, Recall: 0.15, F1: 0.13\n",
            "Validation Loss: 1.8615\n",
            "Validation Precision: 0.04, Recall: 0.14, F1: 0.06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|██████████| 21/21 [01:25<00:00,  4.05s/it, loss=1.88]\n",
            "Validation Epoch 2: 100%|██████████| 6/6 [00:14<00:00,  2.41s/it, loss=1.94]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Training Loss: 1.8791\n",
            "Training Precision: 0.14, Recall: 0.14, F1: 0.13\n",
            "Validation Loss: 1.9357\n",
            "Validation Precision: 0.08, Recall: 0.16, F1: 0.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|██████████| 21/21 [01:39<00:00,  4.75s/it, loss=1.87]\n",
            "Validation Epoch 3: 100%|██████████| 6/6 [00:18<00:00,  3.05s/it, loss=1.91]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Training Loss: 1.8678\n",
            "Training Precision: 0.08, Recall: 0.17, F1: 0.11\n",
            "Validation Loss: 1.9112\n",
            "Validation Precision: 0.08, Recall: 0.15, F1: 0.08\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|██████████| 21/21 [01:27<00:00,  4.17s/it, loss=1.8]\n",
            "Validation Epoch 4: 100%|██████████| 6/6 [00:14<00:00,  2.42s/it, loss=1.87]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Training Loss: 1.8011\n",
            "Training Precision: 0.12, Recall: 0.15, F1: 0.12\n",
            "Validation Loss: 1.8691\n",
            "Validation Precision: 0.07, Recall: 0.15, F1: 0.08\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|██████████| 21/21 [01:34<00:00,  4.50s/it, loss=1.79]\n",
            "Validation Epoch 5: 100%|██████████| 6/6 [00:15<00:00,  2.52s/it, loss=1.76]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Training Loss: 1.7911\n",
            "Training Precision: 0.17, Recall: 0.18, F1: 0.14\n",
            "Validation Loss: 1.7594\n",
            "Validation Precision: 0.35, Recall: 0.35, F1: 0.31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|██████████| 21/21 [01:27<00:00,  4.15s/it, loss=1.75]\n",
            "Validation Epoch 6: 100%|██████████| 6/6 [00:13<00:00,  2.33s/it, loss=1.68]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Training Loss: 1.7520\n",
            "Training Precision: 0.39, Recall: 0.29, F1: 0.31\n",
            "Validation Loss: 1.6752\n",
            "Validation Precision: 0.52, Recall: 0.38, F1: 0.38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|██████████| 21/21 [01:25<00:00,  4.09s/it, loss=1.65]\n",
            "Validation Epoch 7: 100%|██████████| 6/6 [00:16<00:00,  2.83s/it, loss=1.55]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Training Loss: 1.6477\n",
            "Training Precision: 0.39, Recall: 0.27, F1: 0.26\n",
            "Validation Loss: 1.5507\n",
            "Validation Precision: 0.26, Recall: 0.32, F1: 0.28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|██████████| 21/21 [01:26<00:00,  4.11s/it, loss=1.6]\n",
            "Validation Epoch 8: 100%|██████████| 6/6 [00:15<00:00,  2.53s/it, loss=1.49]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Training Loss: 1.6002\n",
            "Training Precision: 0.31, Recall: 0.31, F1: 0.31\n",
            "Validation Loss: 1.4929\n",
            "Validation Precision: 0.31, Recall: 0.37, F1: 0.32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|██████████| 21/21 [01:26<00:00,  4.14s/it, loss=1.6]\n",
            "Validation Epoch 9: 100%|██████████| 6/6 [00:14<00:00,  2.48s/it, loss=1.59]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Training Loss: 1.5990\n",
            "Training Precision: 0.33, Recall: 0.32, F1: 0.32\n",
            "Validation Loss: 1.5914\n",
            "Validation Precision: 0.48, Recall: 0.43, F1: 0.42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|██████████| 21/21 [01:25<00:00,  4.07s/it, loss=1.45]\n",
            "Validation Epoch 10: 100%|██████████| 6/6 [00:15<00:00,  2.65s/it, loss=1.29]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Training Loss: 1.4528\n",
            "Training Precision: 0.46, Recall: 0.40, F1: 0.41\n",
            "Validation Loss: 1.2939\n",
            "Validation Precision: 0.55, Recall: 0.40, F1: 0.39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 原始设置  dropout =0.5\n",
        "train_and_validate(model, criterion, optimizer, train_loader, test_loader, n_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIAJf20DJHsC"
      },
      "source": [
        "batchnorm + dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPtI4Y6QJAdf",
        "outputId": "ebdee306-1f7b-4afa-f664-e6b390cb9ad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DualCNN(\n",
            "  (branch1): Sequential(\n",
            "    (0): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Dropout(p=0.25, inplace=False)\n",
            "    (5): Conv2d(20, 20, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (6): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (9): Dropout(p=0.25, inplace=False)\n",
            "  )\n",
            "  (branch2): Sequential(\n",
            "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Dropout(p=0.25, inplace=False)\n",
            "    (5): Conv2d(20, 20, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (6): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (9): Dropout(p=0.25, inplace=False)\n",
            "  )\n",
            "  (fc1): Linear(in_features=125440, out_features=224, bias=True)\n",
            "  (fc2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=224, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DualCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DualCNN, self).__init__()\n",
        "        # Define the first branch for the RGB image\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 20, kernel_size=5, padding=2),  # Padding=2 to keep size constant\n",
        "            nn.BatchNorm2d(20),  # Batch Normalization after convolution\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25),  # Dropout after pooling\n",
        "\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),  # Larger kernel and padding\n",
        "            nn.BatchNorm2d(20),  # Batch Normalization after convolution\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25)  # Dropout after pooling\n",
        "        )\n",
        "\n",
        "        # Define the second branch for the mask image\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(1, 20, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(20),  # Batch Normalization after convolution\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25),  # Dropout after pooling\n",
        "\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm2d(20),  # Batch Normalization after convolution\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25)  # Dropout after pooling\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(20 * 56 * 56 * 2, 224)\n",
        "        self.fc2 = nn.Dropout(0.5)  # Dropout before the final classification layer\n",
        "        self.fc3 = nn.Linear(224, 7)\n",
        "\n",
        "    def forward(self, x_img, x_mask):\n",
        "        out_img = self.branch1(x_img)\n",
        "        out_mask = self.branch2(x_mask)\n",
        "\n",
        "        out_img = out_img.view(out_img.size(0), -1)\n",
        "        out_mask = out_mask.view(out_mask.size(0), -1)\n",
        "        out = torch.cat((out_img, out_mask), dim=1)\n",
        "\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)  # Applying dropout\n",
        "        out = self.fc3(out)  # Final output layer\n",
        "        return out\n",
        "\n",
        "# Initialize model\n",
        "model = DualCNN()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtoqAmt_JEmn"
      },
      "outputs": [],
      "source": [
        "model = DualCNN()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)   # update the model's parameters (weights and biases) during the training\n",
        "criterion = nn.CrossEntropyLoss()  #define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWc1VBWbJEmo",
        "outputId": "a0d28e57-f816-44ec-dff4-b1e9bcf243d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|██████████| 21/21 [01:30<00:00,  4.29s/it, loss=2.64]\n",
            "Validation Epoch 1: 100%|██████████| 6/6 [00:13<00:00,  2.32s/it, loss=1.94]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Training Loss: 2.6386\n",
            "Training Precision: 0.15, Recall: 0.15, F1: 0.14\n",
            "Validation Loss: 1.9390\n",
            "Validation Precision: 0.07, Recall: 0.14, F1: 0.09\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|██████████| 21/21 [01:28<00:00,  4.19s/it, loss=2]\n",
            "Validation Epoch 2: 100%|██████████| 6/6 [00:13<00:00,  2.28s/it, loss=1.82]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Training Loss: 1.9952\n",
            "Training Precision: 0.28, Recall: 0.23, F1: 0.24\n",
            "Validation Loss: 1.8172\n",
            "Validation Precision: 0.08, Recall: 0.15, F1: 0.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|██████████| 21/21 [01:27<00:00,  4.18s/it, loss=1.87]\n",
            "Validation Epoch 3: 100%|██████████| 6/6 [00:13<00:00,  2.18s/it, loss=1.78]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Training Loss: 1.8730\n",
            "Training Precision: 0.15, Recall: 0.17, F1: 0.16\n",
            "Validation Loss: 1.7782\n",
            "Validation Precision: 0.07, Recall: 0.15, F1: 0.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|██████████| 21/21 [01:27<00:00,  4.16s/it, loss=1.86]\n",
            "Validation Epoch 4: 100%|██████████| 6/6 [00:13<00:00,  2.24s/it, loss=1.8]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Training Loss: 1.8619\n",
            "Training Precision: 0.21, Recall: 0.18, F1: 0.17\n",
            "Validation Loss: 1.8040\n",
            "Validation Precision: 0.30, Recall: 0.25, F1: 0.19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|██████████| 21/21 [01:26<00:00,  4.10s/it, loss=1.81]\n",
            "Validation Epoch 5: 100%|██████████| 6/6 [00:13<00:00,  2.33s/it, loss=1.71]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Training Loss: 1.8091\n",
            "Training Precision: 0.14, Recall: 0.18, F1: 0.15\n",
            "Validation Loss: 1.7135\n",
            "Validation Precision: 0.22, Recall: 0.21, F1: 0.18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|██████████| 21/21 [01:25<00:00,  4.07s/it, loss=1.81]\n",
            "Validation Epoch 6: 100%|██████████| 6/6 [00:13<00:00,  2.28s/it, loss=1.75]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Training Loss: 1.8111\n",
            "Training Precision: 0.11, Recall: 0.16, F1: 0.11\n",
            "Validation Loss: 1.7523\n",
            "Validation Precision: 0.07, Recall: 0.15, F1: 0.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|██████████| 21/21 [01:26<00:00,  4.11s/it, loss=1.8]\n",
            "Validation Epoch 7: 100%|██████████| 6/6 [00:13<00:00,  2.32s/it, loss=1.81]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Training Loss: 1.8026\n",
            "Training Precision: 0.06, Recall: 0.13, F1: 0.08\n",
            "Validation Loss: 1.8079\n",
            "Validation Precision: 0.09, Recall: 0.16, F1: 0.11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|██████████| 21/21 [01:28<00:00,  4.20s/it, loss=1.83]\n",
            "Validation Epoch 8: 100%|██████████| 6/6 [00:13<00:00,  2.25s/it, loss=1.73]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Training Loss: 1.8300\n",
            "Training Precision: 0.11, Recall: 0.13, F1: 0.11\n",
            "Validation Loss: 1.7331\n",
            "Validation Precision: 0.09, Recall: 0.16, F1: 0.11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|██████████| 21/21 [01:32<00:00,  4.40s/it, loss=1.78]\n",
            "Validation Epoch 9: 100%|██████████| 6/6 [00:13<00:00,  2.25s/it, loss=1.79]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Training Loss: 1.7790\n",
            "Training Precision: 0.20, Recall: 0.17, F1: 0.15\n",
            "Validation Loss: 1.7915\n",
            "Validation Precision: 0.06, Recall: 0.15, F1: 0.07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|██████████| 21/21 [01:28<00:00,  4.20s/it, loss=1.79]\n",
            "Validation Epoch 10: 100%|██████████| 6/6 [00:13<00:00,  2.27s/it, loss=1.7]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Training Loss: 1.7921\n",
            "Training Precision: 0.17, Recall: 0.16, F1: 0.15\n",
            "Validation Loss: 1.6996\n",
            "Validation Precision: 0.22, Recall: 0.18, F1: 0.15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# 原始设置  lr=0.001, momentum=0.9\n",
        "train_and_validate(model, criterion, optimizer, train_loader, test_loader, n_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESW3h_B41bYk"
      },
      "source": [
        "### Use Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5JbbIM11rEG"
      },
      "outputs": [],
      "source": [
        "class HandGestureDataset(Dataset):\n",
        "    def __init__(self, rgb_dir, mask_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            rgb_dir (string): Directory with all the RGB images divided into subdirectories.\n",
        "            mask_dir (string): Directory with all the mask images divided into the same subdirectories as RGB images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.rgb_dir = rgb_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.rgb_transform = rgb_transform\n",
        "        self.mask_transform = mask_transform\n",
        "        self.samples = self._load_samples()\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        label_mapping = {'one': 0, 'two': 1, 'three': 2, 'four': 3, 'five': 4, 'like': 5, 'ok': 6}\n",
        "\n",
        "        # Iterate over all categories (e.g., 'one', 'two', etc.)\n",
        "        for category in label_mapping:\n",
        "            rgb_path = os.path.join(self.rgb_dir, category)\n",
        "            mask_path = os.path.join(self.mask_dir, category)\n",
        "            for filename in os.listdir(rgb_path):\n",
        "                if filename.endswith('.jpg'):\n",
        "                    # Constructing the mask filename based on the RGB filename\n",
        "                    # RGB filename like '2_Original_046.jpg' corresponds to mask '2_Mask_046.jpg'\n",
        "                    mask_filename = filename.replace('Original', 'Mask')\n",
        "                    file_rgb_path = os.path.join(rgb_path, filename)\n",
        "                    file_mask_path = os.path.join(mask_path, mask_filename)\n",
        "                    samples.append((file_rgb_path, file_mask_path, label_mapping[category]))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rgb_path, mask_path, label = self.samples[idx]\n",
        "        rgb_image = Image.open(rgb_path).convert('RGB')\n",
        "        mask_image = Image.open(mask_path).convert('L')  # 确保掩码是单通道的\n",
        "\n",
        "        if self.rgb_transform:\n",
        "            rgb_image = self.rgb_transform(rgb_image)\n",
        "        if self.mask_transform:\n",
        "            mask_image = self.mask_transform(mask_image)\n",
        "\n",
        "        return rgb_image, mask_image, label\n",
        "\n",
        "# Transformation to apply\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),  # Resize all images to the same size for model consistency\n",
        "#     transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
        "#     transforms.RandomRotation(15),  # Randomly rotate images by up to 15 degrees\n",
        "#     transforms.ColorJitter(brightness=0.1, contrast=0.1),  # Randomly jitter brightness and contrast\n",
        "#     transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize tensors\n",
        "# ])\n",
        "\n",
        "\n",
        "rgb_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # 保持大小一致\n",
        "    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
        "    transforms.RandomRotation(15),  # 随机旋转\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),  # 调整亮度和对比度\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 归一化\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),  # 掩码也需要翻转以匹配图像的变换\n",
        "    transforms.RandomRotation(15),  # 掩码也需要旋转\n",
        "    transforms.ToTensor()  # 直接转为张量，不需要归一化\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDA6PyZG1rEh",
        "outputId": "43bb6162-963e-4b96-9f98-c3b254b7dab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('/content/drive/MyDrive/CS445/Final Project/rgb/one/1_Original_018.jpg', '/content/drive/MyDrive/CS445/Final Project/generate/one/1_Mask_018.jpg', 0)\n",
            "('/content/drive/MyDrive/CS445/Final Project/rgb/one/1_Original_022.jpg', '/content/drive/MyDrive/CS445/Final Project/generate/one/1_Mask_022.jpg', 0)\n",
            "('/content/drive/MyDrive/CS445/Final Project/rgb/one/1_Original_010.jpg', '/content/drive/MyDrive/CS445/Final Project/generate/one/1_Mask_010.jpg', 0)\n",
            "('/content/drive/MyDrive/CS445/Final Project/rgb/one/1_Original_021.jpg', '/content/drive/MyDrive/CS445/Final Project/generate/one/1_Mask_021.jpg', 0)\n",
            "('/content/drive/MyDrive/CS445/Final Project/rgb/one/1_Original_024.jpg', '/content/drive/MyDrive/CS445/Final Project/generate/one/1_Mask_024.jpg', 0)\n"
          ]
        }
      ],
      "source": [
        "dataset = HandGestureDataset('/content/drive/MyDrive/CS445/Final Project/rgb', '/content/drive/MyDrive/CS445/Final Project/generate', transform=transform)\n",
        "for i in range(5):  # Check first 5 samples\n",
        "    print(dataset.samples[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oQzQJH-1rEi"
      },
      "outputs": [],
      "source": [
        "# hand_dir = '/content/drive/MyDrive/CS445/Final Project/rgb'\n",
        "# mask_dir = '/content/drive/MyDrive/CS445/Final Project/generate'\n",
        "# dataset = HandGestureDataset(hand_dir, mask_dir, transform=transform)\n",
        "\n",
        "# Split the dataset into training and testing\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# DataLoader setup\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDaGaU7K1Zl1",
        "outputId": "513000b6-b8d4-48a5-8848-e334e5b7fe80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DualCNN(\n",
            "  (branch1): Sequential(\n",
            "    (0): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(20, 20, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (branch2): Sequential(\n",
            "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(20, 20, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Linear(in_features=125440, out_features=224, bias=True)\n",
            "  (fc2): Linear(in_features=224, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class DualCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DualCNN, self).__init__()\n",
        "        # Define the first branch for the RGB image\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 20, kernel_size=5, padding=2),  # Padding=2 to keep size constant\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),  # Larger kernel and padding\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Define the second branch for the mask image\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(1, 20, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Define the fully connected layers\n",
        "        self.fc1 = nn.Linear(20 * 56 * 56 * 2, 224)  # calculated\n",
        "        self.fc2 = nn.Linear(224, 7)\n",
        "\n",
        "    def forward(self, x_img, x_mask):\n",
        "        out_img = self.branch1(x_img)\n",
        "        out_mask = self.branch2(x_mask)\n",
        "\n",
        "        # print(\"Output size after branch1:\", out_img.shape)  # Debug: Check output size\n",
        "        # print(\"Output size after branch2:\", out_mask.shape)\n",
        "\n",
        "        out_img = out_img.view(out_img.size(0), -1)\n",
        "        out_mask = out_mask.view(out_mask.size(0), -1)\n",
        "        out = torch.cat((out_img, out_mask), dim=1)\n",
        "\n",
        "        # print(\"Concatenated output size:\", out.shape)  # Debug: Check concatenated size\n",
        "\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Initialize model\n",
        "model = DualCNN()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yan1IHh11Zl2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_validate(model, criterion, optimizer, train_loader, val_loader, n_epochs=10):\n",
        "    for epoch in range(n_epochs):\n",
        "        # Initialize metrics\n",
        "        train_losses, val_losses = [], []\n",
        "        train_preds, train_targets = [], []\n",
        "        val_preds, val_targets = [], []\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_tqdm = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
        "        for data_img, data_mask, labels in train_tqdm:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data_img, data_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss and predictions\n",
        "            train_losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_preds.extend(predicted.cpu().numpy())\n",
        "            train_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            train_tqdm.set_postfix(loss=np.mean(train_losses))\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_tqdm = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}')\n",
        "        with torch.no_grad():\n",
        "            for data_img, data_mask, labels in val_tqdm:\n",
        "                outputs = model(data_img, data_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Accumulate loss and predictions\n",
        "                val_losses.append(loss.item())\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_preds.extend(predicted.cpu().numpy())\n",
        "                val_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                val_tqdm.set_postfix(loss=np.mean(val_losses))\n",
        "\n",
        "        # Calculate metrics for training\n",
        "        train_precision = precision_score(train_targets, train_preds, average='macro')\n",
        "        train_recall = recall_score(train_targets, train_preds, average='macro')\n",
        "        train_f1 = f1_score(train_targets, train_preds, average='macro')\n",
        "\n",
        "        # Calculate metrics for validation\n",
        "        val_precision = precision_score(val_targets, val_preds, average='macro')\n",
        "        val_recall = recall_score(val_targets, val_preds, average='macro')\n",
        "        val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
        "\n",
        "        # End of Epoch Summary\n",
        "        print(f'Epoch {epoch+1}: Training Loss: {np.mean(train_losses):.4f}')\n",
        "        print(f'Training Precision: {train_precision:.2f}, Recall: {train_recall:.2f}, F1: {train_f1:.2f}')\n",
        "        print(f'Validation Loss: {np.mean(val_losses):.4f}')\n",
        "        print(f'Validation Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1: {val_f1:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8-a-aMo1Zl3"
      },
      "outputs": [],
      "source": [
        "model = DualCNN()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)   # update the model's parameters (weights and biases) during the training\n",
        "criterion = nn.CrossEntropyLoss()  #define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCqgSP3Q1Zl3",
        "outputId": "fdde0f56-43fb-474a-8f2b-9457f8470c82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|██████████| 21/21 [01:25<00:00,  4.05s/it, loss=1.87]\n",
            "Validation Epoch 1: 100%|██████████| 6/6 [00:14<00:00,  2.40s/it, loss=1.84]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Training Loss: 1.8749\n",
            "Training Precision: 0.13, Recall: 0.15, F1: 0.13\n",
            "Validation Loss: 1.8356\n",
            "Validation Precision: 0.09, Recall: 0.18, F1: 0.12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|██████████| 21/21 [01:24<00:00,  4.03s/it, loss=1.82]\n",
            "Validation Epoch 2: 100%|██████████| 6/6 [00:16<00:00,  2.77s/it, loss=1.87]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Training Loss: 1.8159\n",
            "Training Precision: 0.11, Recall: 0.18, F1: 0.13\n",
            "Validation Loss: 1.8692\n",
            "Validation Precision: 0.04, Recall: 0.13, F1: 0.06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|██████████| 21/21 [01:24<00:00,  4.03s/it, loss=1.79]\n",
            "Validation Epoch 3: 100%|██████████| 6/6 [00:14<00:00,  2.48s/it, loss=1.76]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Training Loss: 1.7855\n",
            "Training Precision: 0.26, Recall: 0.19, F1: 0.16\n",
            "Validation Loss: 1.7585\n",
            "Validation Precision: 0.32, Recall: 0.27, F1: 0.23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|██████████| 21/21 [01:23<00:00,  3.98s/it, loss=1.77]\n",
            "Validation Epoch 4: 100%|██████████| 6/6 [00:13<00:00,  2.30s/it, loss=1.69]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Training Loss: 1.7683\n",
            "Training Precision: 0.25, Recall: 0.21, F1: 0.20\n",
            "Validation Loss: 1.6888\n",
            "Validation Precision: 0.33, Recall: 0.31, F1: 0.25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|██████████| 21/21 [01:25<00:00,  4.05s/it, loss=1.66]\n",
            "Validation Epoch 5: 100%|██████████| 6/6 [00:13<00:00,  2.30s/it, loss=1.49]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Training Loss: 1.6606\n",
            "Training Precision: 0.44, Recall: 0.31, F1: 0.28\n",
            "Validation Loss: 1.4938\n",
            "Validation Precision: 0.16, Recall: 0.28, F1: 0.18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|██████████| 21/21 [01:22<00:00,  3.95s/it, loss=1.58]\n",
            "Validation Epoch 6: 100%|██████████| 6/6 [00:13<00:00,  2.28s/it, loss=1.51]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Training Loss: 1.5826\n",
            "Training Precision: 0.29, Recall: 0.27, F1: 0.25\n",
            "Validation Loss: 1.5051\n",
            "Validation Precision: 0.36, Recall: 0.33, F1: 0.31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|██████████| 21/21 [01:23<00:00,  3.97s/it, loss=1.61]\n",
            "Validation Epoch 7: 100%|██████████| 6/6 [00:13<00:00,  2.32s/it, loss=1.57]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Training Loss: 1.6080\n",
            "Training Precision: 0.40, Recall: 0.35, F1: 0.36\n",
            "Validation Loss: 1.5667\n",
            "Validation Precision: 0.15, Recall: 0.35, F1: 0.21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|██████████| 21/21 [01:21<00:00,  3.88s/it, loss=1.57]\n",
            "Validation Epoch 8: 100%|██████████| 6/6 [00:14<00:00,  2.35s/it, loss=1.72]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Training Loss: 1.5689\n",
            "Training Precision: 0.45, Recall: 0.29, F1: 0.27\n",
            "Validation Loss: 1.7211\n",
            "Validation Precision: 0.45, Recall: 0.34, F1: 0.33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|██████████| 21/21 [01:20<00:00,  3.84s/it, loss=1.44]\n",
            "Validation Epoch 9: 100%|██████████| 6/6 [00:15<00:00,  2.57s/it, loss=1.36]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Training Loss: 1.4419\n",
            "Training Precision: 0.43, Recall: 0.36, F1: 0.37\n",
            "Validation Loss: 1.3568\n",
            "Validation Precision: 0.48, Recall: 0.34, F1: 0.28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|██████████| 21/21 [01:20<00:00,  3.85s/it, loss=1.39]\n",
            "Validation Epoch 10: 100%|██████████| 6/6 [00:13<00:00,  2.28s/it, loss=1.47]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Training Loss: 1.3852\n",
            "Training Precision: 0.49, Recall: 0.44, F1: 0.45\n",
            "Validation Loss: 1.4690\n",
            "Validation Precision: 0.29, Recall: 0.40, F1: 0.30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# 原始设置\n",
        "train_and_validate(model, criterion, optimizer, train_loader, test_loader, n_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voSiefw5H4dn"
      },
      "source": [
        "### Hyperparameter tuning （没整）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_3-mTORKJoc"
      },
      "outputs": [],
      "source": [
        "class DualCNN(nn.Module):\n",
        "    def __init__(self, num_filters=20, kernel_size1=5, kernel_size2=7, padding1=2, padding2=3):\n",
        "        super(DualCNN, self).__init__()\n",
        "        # First branch for the RGB image\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(3, num_filters, kernel_size=kernel_size1, padding=padding1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(num_filters, num_filters, kernel_size=kernel_size2, padding=padding2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Second branch for the mask image\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(1, num_filters, kernel_size=kernel_size1, padding=padding1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(num_filters, num_filters, kernel_size=kernel_size2, padding=padding2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(num_filters * 56 * 56 * 2, 224)  # Adjust size calculation as necessary\n",
        "        self.fc2 = nn.Linear(224, 7)\n",
        "\n",
        "    def forward(self, x_img, x_mask):\n",
        "        out_img = self.branch1(x_img)\n",
        "        out_mask = self.branch2(x_mask)\n",
        "\n",
        "        out_img = out_img.view(out_img.size(0), -1)\n",
        "        out_mask = out_mask.view(out_mask.size(0), -1)\n",
        "        out = torch.cat((out_img, out_mask), dim=1)\n",
        "\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "rbXWRLxW4DWn",
        "outputId": "15d62c5e-0fbf-4fbd-fb86-af2907906864"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-05-03 20:32:36,911] A new study created in memory with name: no-name-af2aee04-8f52-43d3-8139-44a75cd0bb36\n",
            "[W 2024-05-03 20:33:17,104] Trial 0 failed with parameters: {'lr': 0.00039582641463206545, 'num_filters': 16, 'kernel_size1': 5} because of the following error: NameError(\"name 'evaluate' is not defined\").\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-36-58233ae8c4dd>\", line 25, in objective\n",
            "    accuracy = evaluate(model, val_loader)  # Define a function to calculate validation accuracy\n",
            "NameError: name 'evaluate' is not defined\n",
            "[W 2024-05-03 20:33:17,106] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'evaluate' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-58233ae8c4dd>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best hyperparameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     ):\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-58233ae8c4dd>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Validation phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Define a function to calculate validation accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m  \u001b[0;31m# Objective value to maximize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import torch.optim as optim\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    num_filters = trial.suggest_categorical(\"num_filters\", [16, 32, 64])\n",
        "    kernel_size1 = trial.suggest_categorical(\"kernel_size1\", [3, 5, 7])\n",
        "\n",
        "    model = DualCNN(num_filters=num_filters, kernel_size1=kernel_size1)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(10):  # Reduced number of epochs for quick tuning\n",
        "        model.train()\n",
        "        for data_img, data_mask, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data_img, data_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        accuracy = evaluate(model, val_loader)  # Define a function to calculate validation accuracy\n",
        "\n",
        "    return accuracy  # Objective value to maximize\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "print(\"Best hyperparameters:\", study.best_trial.params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5SJZgsy4E-R"
      },
      "source": [
        "### DC CNN + ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGfTqybfOyuT"
      },
      "outputs": [],
      "source": [
        "class AttentionGate(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(AttentionGate, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi\n",
        "\n",
        "class DualCNN_attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DualCNN_attention, self).__init__()\n",
        "        # Define the first branch for the RGB image\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 20, kernel_size=5, padding=2),  # Padding=2 to keep size constant\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),  # Larger kernel and padding\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Define the second branch for the mask image\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(1, 20, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(20, 20, kernel_size=7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Attention Gate\n",
        "        self.attention_gate = AttentionGate(F_g=20, F_l=20, F_int=10)\n",
        "\n",
        "        # Define the fully connected layers\n",
        "        self.fc1 = nn.Linear(20 * 56 * 56 * 2, 224)  # calculated\n",
        "        self.fc2 = nn.Linear(224, 7)\n",
        "\n",
        "    def forward(self, x_img, x_mask):\n",
        "        out_img = self.branch1(x_img)\n",
        "        out_mask = self.branch2(x_mask)\n",
        "\n",
        "        # Apply attention\n",
        "        out_mask = self.attention_gate(out_img, out_mask)\n",
        "\n",
        "        out_img = out_img.view(out_img.size(0), -1)\n",
        "        out_mask = out_mask.view(out_mask.size(0), -1)\n",
        "        out = torch.cat((out_img, out_mask), dim=1)\n",
        "\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei0095QPO-T5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOewAWX6PKGL"
      },
      "outputs": [],
      "source": [
        "model = DualCNN_attention()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)   # update the model's parameters (weights and biases) during the training\n",
        "criterion = nn.CrossEntropyLoss()  #define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cEN-Z40PKGM",
        "outputId": "280aa060-7483-4c1a-b9a1-9c3d6a53ae26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|██████████| 21/21 [01:34<00:00,  4.49s/it, loss=1.9]\n",
            "Validation Epoch 1: 100%|██████████| 6/6 [00:20<00:00,  3.40s/it, loss=1.87]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Training Loss: 1.8974\n",
            "Training Precision: 0.12, Recall: 0.15, F1: 0.12\n",
            "Validation Loss: 1.8740\n",
            "Validation Precision: 0.03, Recall: 0.14, F1: 0.05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|██████████| 21/21 [01:29<00:00,  4.25s/it, loss=1.78]\n",
            "Validation Epoch 2: 100%|██████████| 6/6 [00:16<00:00,  2.69s/it, loss=1.57]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Training Loss: 1.7784\n",
            "Training Precision: 0.31, Recall: 0.22, F1: 0.20\n",
            "Validation Loss: 1.5694\n",
            "Validation Precision: 0.37, Recall: 0.29, F1: 0.25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|██████████| 21/21 [01:32<00:00,  4.41s/it, loss=1.63]\n",
            "Validation Epoch 3: 100%|██████████| 6/6 [00:17<00:00,  2.89s/it, loss=1.74]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Training Loss: 1.6263\n",
            "Training Precision: 0.38, Recall: 0.37, F1: 0.37\n",
            "Validation Loss: 1.7401\n",
            "Validation Precision: 0.22, Recall: 0.19, F1: 0.15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|██████████| 21/21 [01:32<00:00,  4.41s/it, loss=1.51]\n",
            "Validation Epoch 4: 100%|██████████| 6/6 [00:16<00:00,  2.77s/it, loss=1.57]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Training Loss: 1.5110\n",
            "Training Precision: 0.37, Recall: 0.37, F1: 0.36\n",
            "Validation Loss: 1.5689\n",
            "Validation Precision: 0.41, Recall: 0.29, F1: 0.31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|██████████| 21/21 [01:32<00:00,  4.38s/it, loss=1.44]\n",
            "Validation Epoch 5: 100%|██████████| 6/6 [00:16<00:00,  2.72s/it, loss=1.6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Training Loss: 1.4430\n",
            "Training Precision: 0.49, Recall: 0.44, F1: 0.46\n",
            "Validation Loss: 1.5979\n",
            "Validation Precision: 0.37, Recall: 0.31, F1: 0.29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|██████████| 21/21 [01:30<00:00,  4.30s/it, loss=1.26]\n",
            "Validation Epoch 6: 100%|██████████| 6/6 [00:16<00:00,  2.76s/it, loss=2.25]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Training Loss: 1.2640\n",
            "Training Precision: 0.59, Recall: 0.52, F1: 0.54\n",
            "Validation Loss: 2.2527\n",
            "Validation Precision: 0.26, Recall: 0.35, F1: 0.27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|██████████| 21/21 [01:40<00:00,  4.79s/it, loss=1.3]\n",
            "Validation Epoch 7: 100%|██████████| 6/6 [00:17<00:00,  2.90s/it, loss=2.11]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Training Loss: 1.3042\n",
            "Training Precision: 0.53, Recall: 0.51, F1: 0.51\n",
            "Validation Loss: 2.1095\n",
            "Validation Precision: 0.18, Recall: 0.34, F1: 0.22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|██████████| 21/21 [01:34<00:00,  4.51s/it, loss=1.11]\n",
            "Validation Epoch 8: 100%|██████████| 6/6 [00:16<00:00,  2.77s/it, loss=2.03]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Training Loss: 1.1100\n",
            "Training Precision: 0.63, Recall: 0.55, F1: 0.57\n",
            "Validation Loss: 2.0338\n",
            "Validation Precision: 0.22, Recall: 0.36, F1: 0.26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|██████████| 21/21 [01:29<00:00,  4.28s/it, loss=1.04]\n",
            "Validation Epoch 9: 100%|██████████| 6/6 [00:16<00:00,  2.75s/it, loss=1.79]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Training Loss: 1.0383\n",
            "Training Precision: 0.64, Recall: 0.57, F1: 0.59\n",
            "Validation Loss: 1.7899\n",
            "Validation Precision: 0.33, Recall: 0.36, F1: 0.32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|██████████| 21/21 [01:32<00:00,  4.39s/it, loss=0.672]\n",
            "Validation Epoch 10: 100%|██████████| 6/6 [00:16<00:00,  2.79s/it, loss=2.07]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Training Loss: 0.6715\n",
            "Training Precision: 0.76, Recall: 0.75, F1: 0.75\n",
            "Validation Loss: 2.0661\n",
            "Validation Precision: 0.35, Recall: 0.43, F1: 0.32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_and_validate(model, criterion, optimizer, train_loader, test_loader, n_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DC + ResNet18 (LIGHTWEIGHT)"
      ],
      "metadata": {
        "id": "pXvgpINikGjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DualResNet(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(DualResNet, self).__init__()\n",
        "\n",
        "        # Initialize ResNet backbones\n",
        "        self.resnet_branch1 = models.resnet18(pretrained=True)  # pre-trained on imagenet\n",
        "        self.resnet_branch1.fc = nn.Identity()\n",
        "\n",
        "        self.resnet_branch2 = models.resnet18(pretrained=True)\n",
        "        self.resnet_branch2.fc = nn.Identity()\n",
        "\n",
        "        # Modify the first convolution layer of the second branch to accept 1-channel input\n",
        "        self.resnet_branch2.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(512 * 2, 224)  # the output of each ResNet is 512 features\n",
        "        self.fc2 = nn.Linear(224, num_classes)\n",
        "\n",
        "    def forward(self, x_img, x_mask):\n",
        "        # Process inputs through each ResNet branch\n",
        "        out_img = self.resnet_branch1(x_img)\n",
        "        out_mask = self.resnet_branch2(x_mask)\n",
        "\n",
        "        # Concatenate features from both branches\n",
        "        out = torch.cat((out_img, out_mask), dim=1)\n",
        "\n",
        "        # Pass through fully connected layers\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Initialize and print model\n",
        "model = DualResNet()\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q8vCHWokI85",
        "outputId": "d80a86dc-3ad7-4c64-b39c-fe6219d1d726"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DualResNet(\n",
            "  (resnet_branch1): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Identity()\n",
            "  )\n",
            "  (resnet_branch2): ResNet(\n",
            "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Identity()\n",
            "  )\n",
            "  (fc1): Linear(in_features=1024, out_features=224, bias=True)\n",
            "  (fc2): Linear(in_features=224, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "lqHGjDvbkVaA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_validate(model, criterion, optimizer, train_loader, val_loader, n_epochs=10):\n",
        "    for epoch in range(n_epochs):\n",
        "        # Initialize metrics\n",
        "        train_losses, val_losses = [], []\n",
        "        train_preds, train_targets = [], []\n",
        "        val_preds, val_targets = [], []\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_tqdm = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
        "        for data_img, data_mask, labels in train_tqdm:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data_img, data_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss and predictions\n",
        "            train_losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_preds.extend(predicted.cpu().numpy())\n",
        "            train_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            train_tqdm.set_postfix(loss=np.mean(train_losses))\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_tqdm = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}')\n",
        "        with torch.no_grad():\n",
        "            for data_img, data_mask, labels in val_tqdm:\n",
        "                outputs = model(data_img, data_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Accumulate loss and predictions\n",
        "                val_losses.append(loss.item())\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_preds.extend(predicted.cpu().numpy())\n",
        "                val_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                val_tqdm.set_postfix(loss=np.mean(val_losses))\n",
        "\n",
        "        # Calculate metrics for training\n",
        "        train_precision = precision_score(train_targets, train_preds, average='macro')\n",
        "        train_recall = recall_score(train_targets, train_preds, average='macro')\n",
        "        train_f1 = f1_score(train_targets, train_preds, average='macro')\n",
        "\n",
        "        # Calculate metrics for validation\n",
        "        val_precision = precision_score(val_targets, val_preds, average='macro')\n",
        "        val_recall = recall_score(val_targets, val_preds, average='macro')\n",
        "        val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
        "\n",
        "        # End of Epoch Summary\n",
        "        print(f'Epoch {epoch+1}: Training Loss: {np.mean(train_losses):.4f}')\n",
        "        print(f'Training Precision: {train_precision:.2f}, Recall: {train_recall:.2f}, F1: {train_f1:.2f}')\n",
        "        print(f'Validation Loss: {np.mean(val_losses):.4f}')\n",
        "        print(f'Validation Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1: {val_f1:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "D0YVOSgGkVaN"
      },
      "outputs": [],
      "source": [
        "model = DualResNet()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)   # update the model's parameters (weights and biases) during the training\n",
        "criterion = nn.CrossEntropyLoss()  #define loss function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate(model, criterion, optimizer, train_loader, test_loader, n_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn_tkiQOkg8L",
        "outputId": "e4bd86df-076c-418e-9b94-307068d0a0d8"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 21/21 [02:51<00:00,  8.17s/it, loss=1.8]\n",
            "Validation Epoch 1: 100%|██████████| 6/6 [00:22<00:00,  3.70s/it, loss=1.97]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Training Loss: 1.8014\n",
            "Training Precision: 0.25, Recall: 0.20, F1: 0.20\n",
            "Validation Loss: 1.9747\n",
            "Validation Precision: 0.09, Recall: 0.15, F1: 0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 21/21 [02:42<00:00,  7.72s/it, loss=1.28]\n",
            "Validation Epoch 2: 100%|██████████| 6/6 [00:19<00:00,  3.25s/it, loss=1.42]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Training Loss: 1.2844\n",
            "Training Precision: 0.55, Recall: 0.44, F1: 0.43\n",
            "Validation Loss: 1.4238\n",
            "Validation Precision: 0.50, Recall: 0.51, F1: 0.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 21/21 [02:43<00:00,  7.80s/it, loss=0.918]\n",
            "Validation Epoch 3: 100%|██████████| 6/6 [00:20<00:00,  3.44s/it, loss=1.95]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Training Loss: 0.9180\n",
            "Training Precision: 0.62, Recall: 0.60, F1: 0.60\n",
            "Validation Loss: 1.9499\n",
            "Validation Precision: 0.43, Recall: 0.40, F1: 0.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 21/21 [02:41<00:00,  7.68s/it, loss=1.13]\n",
            "Validation Epoch 4: 100%|██████████| 6/6 [00:21<00:00,  3.57s/it, loss=1.17]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Training Loss: 1.1297\n",
            "Training Precision: 0.62, Recall: 0.59, F1: 0.60\n",
            "Validation Loss: 1.1745\n",
            "Validation Precision: 0.47, Recall: 0.56, F1: 0.49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 21/21 [02:43<00:00,  7.78s/it, loss=1.03]\n",
            "Validation Epoch 5: 100%|██████████| 6/6 [00:19<00:00,  3.19s/it, loss=1.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Training Loss: 1.0345\n",
            "Training Precision: 0.64, Recall: 0.63, F1: 0.63\n",
            "Validation Loss: 1.1539\n",
            "Validation Precision: 0.52, Recall: 0.55, F1: 0.52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 21/21 [02:43<00:00,  7.79s/it, loss=1.12]\n",
            "Validation Epoch 6: 100%|██████████| 6/6 [00:19<00:00,  3.23s/it, loss=1.13]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Training Loss: 1.1164\n",
            "Training Precision: 0.62, Recall: 0.65, F1: 0.63\n",
            "Validation Loss: 1.1285\n",
            "Validation Precision: 0.65, Recall: 0.61, F1: 0.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 21/21 [02:43<00:00,  7.79s/it, loss=0.709]\n",
            "Validation Epoch 7: 100%|██████████| 6/6 [00:20<00:00,  3.43s/it, loss=1.54]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Training Loss: 0.7092\n",
            "Training Precision: 0.73, Recall: 0.74, F1: 0.73\n",
            "Validation Loss: 1.5393\n",
            "Validation Precision: 0.55, Recall: 0.58, F1: 0.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 21/21 [02:40<00:00,  7.65s/it, loss=0.986]\n",
            "Validation Epoch 8: 100%|██████████| 6/6 [00:20<00:00,  3.42s/it, loss=1.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Training Loss: 0.9857\n",
            "Training Precision: 0.67, Recall: 0.65, F1: 0.66\n",
            "Validation Loss: 1.0948\n",
            "Validation Precision: 0.61, Recall: 0.53, F1: 0.53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 21/21 [02:44<00:00,  7.81s/it, loss=0.559]\n",
            "Validation Epoch 9: 100%|██████████| 6/6 [00:24<00:00,  4.08s/it, loss=2.21]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Training Loss: 0.5594\n",
            "Training Precision: 0.83, Recall: 0.83, F1: 0.83\n",
            "Validation Loss: 2.2094\n",
            "Validation Precision: 0.44, Recall: 0.47, F1: 0.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 21/21 [02:51<00:00,  8.18s/it, loss=0.815]\n",
            "Validation Epoch 10: 100%|██████████| 6/6 [00:20<00:00,  3.46s/it, loss=0.77]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Training Loss: 0.8154\n",
            "Training Precision: 0.75, Recall: 0.75, F1: 0.75\n",
            "Validation Loss: 0.7703\n",
            "Validation Precision: 0.64, Recall: 0.64, F1: 0.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DC + VGG 19"
      ],
      "metadata": {
        "id": "eBOt0Ncs6XGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DualVGG(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(DualVGG, self).__init__()\n",
        "\n",
        "        # Load pre-trained VGG16 for the first branch (handling RGB images)\n",
        "        base_model1 = models.vgg16(pretrained=True)\n",
        "        # Correctly handling the avgpool\n",
        "        self.vgg_branch1 = nn.Sequential(\n",
        "            *base_model1.features,\n",
        "            base_model1.avgpool,\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Load pre-trained VGG16 for the second branch (handling mask images)\n",
        "        base_model2 = models.vgg16(pretrained=True)\n",
        "        # Modify the first convolutional layer to accept 1-channel input\n",
        "        base_model2.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        # Correctly handling the avgpool\n",
        "        self.vgg_branch2 = nn.Sequential(\n",
        "            *base_model2.features,\n",
        "            base_model2.avgpool,\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Define the classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7 * 2, 4096),  # Adjust based on the output size from the avgpool layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)  # Outputs for 7 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x_img, x_mask):\n",
        "        # Process inputs through each VGG branch\n",
        "        x_img = self.vgg_branch1(x_img)\n",
        "        x_mask = self.vgg_branch2(x_mask)\n",
        "\n",
        "        # Concatenate features from both branches\n",
        "        x = torch.cat((x_img, x_mask), dim=1)\n",
        "\n",
        "        # Classification\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and print model\n",
        "model = DualVGG(num_classes=7)  # Explicitly setting number of classes to 7\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjyHXpJ86d7d",
        "outputId": "92825411-ef7c-4fe8-e97d-9e12cc9b1a2a"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DualVGG(\n",
            "  (vgg_branch1): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (31): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "    (32): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (vgg_branch2): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (31): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "    (32): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=50176, out_features=4096, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=7, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "532nu1-s7G53"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_validate(model, criterion, optimizer, train_loader, val_loader, n_epochs=10):\n",
        "    for epoch in range(n_epochs):\n",
        "        # Initialize metrics\n",
        "        train_losses, val_losses = [], []\n",
        "        train_preds, train_targets = [], []\n",
        "        val_preds, val_targets = [], []\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_tqdm = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
        "        for data_img, data_mask, labels in train_tqdm:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data_img, data_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss and predictions\n",
        "            train_losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_preds.extend(predicted.cpu().numpy())\n",
        "            train_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            train_tqdm.set_postfix(loss=np.mean(train_losses))\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_tqdm = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}')\n",
        "        with torch.no_grad():\n",
        "            for data_img, data_mask, labels in val_tqdm:\n",
        "                outputs = model(data_img, data_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Accumulate loss and predictions\n",
        "                val_losses.append(loss.item())\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_preds.extend(predicted.cpu().numpy())\n",
        "                val_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                val_tqdm.set_postfix(loss=np.mean(val_losses))\n",
        "\n",
        "        # Calculate metrics for training\n",
        "        train_precision = precision_score(train_targets, train_preds, average='macro')\n",
        "        train_recall = recall_score(train_targets, train_preds, average='macro')\n",
        "        train_f1 = f1_score(train_targets, train_preds, average='macro')\n",
        "\n",
        "        # Calculate metrics for validation\n",
        "        val_precision = precision_score(val_targets, val_preds, average='macro')\n",
        "        val_recall = recall_score(val_targets, val_preds, average='macro')\n",
        "        val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
        "\n",
        "        # End of Epoch Summary\n",
        "        print(f'Epoch {epoch+1}: Training Loss: {np.mean(train_losses):.4f}')\n",
        "        print(f'Training Precision: {train_precision:.2f}, Recall: {train_recall:.2f}, F1: {train_f1:.2f}')\n",
        "        print(f'Validation Loss: {np.mean(val_losses):.4f}')\n",
        "        print(f'Validation Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1: {val_f1:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "PGZMPX0d7G54"
      },
      "outputs": [],
      "source": [
        "model = DualVGG()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)   # update the model's parameters (weights and biases) during the training\n",
        "criterion = nn.CrossEntropyLoss()  #define loss function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate(model, criterion, optimizer, train_loader, test_loader, n_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb170843-3ba8-456c-e768-8f8f8a6ba14c",
        "id": "T4rciXDU7G55"
      },
      "execution_count": 159,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|██████████| 21/21 [16:30<00:00, 47.16s/it, loss=1.92]\n",
            "Validation Epoch 1: 100%|██████████| 6/6 [01:24<00:00, 14.02s/it, loss=1.86]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Training Loss: 1.9186\n",
            "Training Precision: 0.19, Recall: 0.14, F1: 0.10\n",
            "Validation Loss: 1.8608\n",
            "Validation Precision: 0.07, Recall: 0.18, F1: 0.10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|██████████| 21/21 [16:02<00:00, 45.85s/it, loss=1.76]\n",
            "Validation Epoch 2: 100%|██████████| 6/6 [01:14<00:00, 12.44s/it, loss=1.67]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Training Loss: 1.7553\n",
            "Training Precision: 0.26, Recall: 0.20, F1: 0.16\n",
            "Validation Loss: 1.6705\n",
            "Validation Precision: 0.49, Recall: 0.39, F1: 0.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 21/21 [15:39<00:00, 44.73s/it, loss=1.48]\n",
            "Validation Epoch 3: 100%|██████████| 6/6 [01:16<00:00, 12.79s/it, loss=1.04]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Training Loss: 1.4776\n",
            "Training Precision: 0.45, Recall: 0.36, F1: 0.37\n",
            "Validation Loss: 1.0381\n",
            "Validation Precision: 0.56, Recall: 0.57, F1: 0.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 21/21 [15:40<00:00, 44.76s/it, loss=1.16]\n",
            "Validation Epoch 4: 100%|██████████| 6/6 [01:15<00:00, 12.52s/it, loss=1.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Training Loss: 1.1562\n",
            "Training Precision: 0.61, Recall: 0.54, F1: 0.57\n",
            "Validation Loss: 1.0388\n",
            "Validation Precision: 0.51, Recall: 0.61, F1: 0.54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 21/21 [15:38<00:00, 44.67s/it, loss=0.991]\n",
            "Validation Epoch 5: 100%|██████████| 6/6 [01:17<00:00, 12.86s/it, loss=0.638]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Training Loss: 0.9905\n",
            "Training Precision: 0.69, Recall: 0.66, F1: 0.67\n",
            "Validation Loss: 0.6381\n",
            "Validation Precision: 0.66, Recall: 0.71, F1: 0.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 21/21 [15:38<00:00, 44.68s/it, loss=0.657]\n",
            "Validation Epoch 6: 100%|██████████| 6/6 [01:16<00:00, 12.67s/it, loss=0.84]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Training Loss: 0.6570\n",
            "Training Precision: 0.80, Recall: 0.79, F1: 0.80\n",
            "Validation Loss: 0.8402\n",
            "Validation Precision: 0.75, Recall: 0.73, F1: 0.71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 21/21 [15:37<00:00, 44.65s/it, loss=0.616]\n",
            "Validation Epoch 7: 100%|██████████| 6/6 [01:15<00:00, 12.54s/it, loss=0.873]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Training Loss: 0.6157\n",
            "Training Precision: 0.81, Recall: 0.73, F1: 0.76\n",
            "Validation Loss: 0.8732\n",
            "Validation Precision: 0.64, Recall: 0.67, F1: 0.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 21/21 [15:35<00:00, 44.55s/it, loss=0.595]\n",
            "Validation Epoch 8: 100%|██████████| 6/6 [01:16<00:00, 12.69s/it, loss=0.805]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Training Loss: 0.5952\n",
            "Training Precision: 0.80, Recall: 0.79, F1: 0.79\n",
            "Validation Loss: 0.8048\n",
            "Validation Precision: 0.67, Recall: 0.71, F1: 0.68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 21/21 [15:43<00:00, 44.91s/it, loss=0.474]\n",
            "Validation Epoch 9: 100%|██████████| 6/6 [01:16<00:00, 12.76s/it, loss=0.715]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Training Loss: 0.4744\n",
            "Training Precision: 0.88, Recall: 0.86, F1: 0.87\n",
            "Validation Loss: 0.7152\n",
            "Validation Precision: 0.79, Recall: 0.81, F1: 0.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 21/21 [15:48<00:00, 45.15s/it, loss=0.371]\n",
            "Validation Epoch 10: 100%|██████████| 6/6 [01:16<00:00, 12.81s/it, loss=0.318]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Training Loss: 0.3706\n",
            "Training Precision: 0.88, Recall: 0.87, F1: 0.88\n",
            "Validation Loss: 0.3183\n",
            "Validation Precision: 0.87, Recall: 0.84, F1: 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8IDeKPMD8c2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DC+ DenseNet"
      ],
      "metadata": {
        "id": "7MV2kC-a8df2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DualDenseNet(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(DualDenseNet, self).__init__()\n",
        "\n",
        "        # Load pre-trained DenseNet121 for the first branch (handling RGB images)\n",
        "        base_model1 = models.densenet121(pretrained=True)\n",
        "        self.dense_branch1 = nn.Sequential(*list(base_model1.features), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten())\n",
        "\n",
        "        # Load pre-trained DenseNet121 for the second branch (handling mask images)\n",
        "        base_model2 = models.densenet121(pretrained=True)\n",
        "        # Modify the first convolutional layer to accept 1-channel input\n",
        "        base_model2.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.dense_branch2 = nn.Sequential(*list(base_model2.features), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten())\n",
        "\n",
        "        # Define the classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024 * 2, 512),  # DenseNet121 outputs 1024 features, concatenated from both branches\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_img, x_mask):\n",
        "        # Process inputs through each DenseNet branch\n",
        "        x_img = self.dense_branch1(x_img)\n",
        "        x_mask = self.dense_branch2(x_mask)\n",
        "\n",
        "        # Concatenate features from both branches\n",
        "        x = torch.cat((x_img, x_mask), dim=1)\n",
        "\n",
        "        # Classification\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and print model\n",
        "model = DualDenseNet(num_classes=7)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWoMD7hn8hk8",
        "outputId": "472dc3cd-0726-47da-ea68-972ca46e36e0"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 82.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DualDenseNet(\n",
            "  (dense_branch1): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (5): _Transition(\n",
            "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (6): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (7): _Transition(\n",
            "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (8): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer13): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer14): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer15): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer16): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer17): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer18): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer19): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer20): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer21): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer22): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer23): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer24): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (9): _Transition(\n",
            "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (10): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer13): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer14): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer15): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer16): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (11): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (14): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (dense_branch2): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (5): _Transition(\n",
            "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (6): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (7): _Transition(\n",
            "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (8): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer13): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer14): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer15): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer16): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer17): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer18): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer19): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer20): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer21): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer22): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer23): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer24): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (9): _Transition(\n",
            "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (10): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer13): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer14): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer15): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer16): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (11): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (14): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=7, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "m32sXjmO8l0-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_validate(model, criterion, optimizer, train_loader, val_loader, n_epochs=10):\n",
        "    for epoch in range(n_epochs):\n",
        "        # Initialize metrics\n",
        "        train_losses, val_losses = [], []\n",
        "        train_preds, train_targets = [], []\n",
        "        val_preds, val_targets = [], []\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_tqdm = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
        "        for data_img, data_mask, labels in train_tqdm:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data_img, data_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss and predictions\n",
        "            train_losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_preds.extend(predicted.cpu().numpy())\n",
        "            train_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            train_tqdm.set_postfix(loss=np.mean(train_losses))\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_tqdm = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}')\n",
        "        with torch.no_grad():\n",
        "            for data_img, data_mask, labels in val_tqdm:\n",
        "                outputs = model(data_img, data_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Accumulate loss and predictions\n",
        "                val_losses.append(loss.item())\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_preds.extend(predicted.cpu().numpy())\n",
        "                val_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                val_tqdm.set_postfix(loss=np.mean(val_losses))\n",
        "\n",
        "        # Calculate metrics for training\n",
        "        train_precision = precision_score(train_targets, train_preds, average='macro')\n",
        "        train_recall = recall_score(train_targets, train_preds, average='macro')\n",
        "        train_f1 = f1_score(train_targets, train_preds, average='macro')\n",
        "\n",
        "        # Calculate metrics for validation\n",
        "        val_precision = precision_score(val_targets, val_preds, average='macro')\n",
        "        val_recall = recall_score(val_targets, val_preds, average='macro')\n",
        "        val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
        "\n",
        "        # End of Epoch Summary\n",
        "        print(f'Epoch {epoch+1}: Training Loss: {np.mean(train_losses):.4f}')\n",
        "        print(f'Training Precision: {train_precision:.2f}, Recall: {train_recall:.2f}, F1: {train_f1:.2f}')\n",
        "        print(f'Validation Loss: {np.mean(val_losses):.4f}')\n",
        "        print(f'Validation Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1: {val_f1:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "fheNzWnd8l1K"
      },
      "outputs": [],
      "source": [
        "model = DualDenseNet()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)   # update the model's parameters (weights and biases) during the training\n",
        "criterion = nn.CrossEntropyLoss()  #define loss function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate(model, criterion, optimizer, train_loader, test_loader, n_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3833ee4b-2e04-4a85-ca39-c2e386832fa3",
        "id": "bElSB2sq8l1K"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 21/21 [04:35<00:00, 13.12s/it, loss=1.93]\n",
            "Validation Epoch 1: 100%|██████████| 6/6 [00:28<00:00,  4.68s/it, loss=1.9]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Training Loss: 1.9310\n",
            "Training Precision: 0.09, Recall: 0.10, F1: 0.09\n",
            "Validation Loss: 1.9030\n",
            "Validation Precision: 0.14, Recall: 0.22, F1: 0.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 21/21 [04:37<00:00, 13.20s/it, loss=1.77]\n",
            "Validation Epoch 2: 100%|██████████| 6/6 [00:31<00:00,  5.29s/it, loss=1.81]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Training Loss: 1.7738\n",
            "Training Precision: 0.18, Recall: 0.16, F1: 0.12\n",
            "Validation Loss: 1.8092\n",
            "Validation Precision: 0.25, Recall: 0.22, F1: 0.18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 21/21 [04:37<00:00, 13.24s/it, loss=1.67]\n",
            "Validation Epoch 3: 100%|██████████| 6/6 [00:27<00:00,  4.65s/it, loss=1.72]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Training Loss: 1.6656\n",
            "Training Precision: 0.43, Recall: 0.28, F1: 0.29\n",
            "Validation Loss: 1.7209\n",
            "Validation Precision: 0.23, Recall: 0.25, F1: 0.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 21/21 [04:35<00:00, 13.10s/it, loss=1.53]\n",
            "Validation Epoch 4: 100%|██████████| 6/6 [00:27<00:00,  4.63s/it, loss=1.47]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Training Loss: 1.5300\n",
            "Training Precision: 0.41, Recall: 0.36, F1: 0.34\n",
            "Validation Loss: 1.4742\n",
            "Validation Precision: 0.51, Recall: 0.48, F1: 0.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 21/21 [04:35<00:00, 13.11s/it, loss=1.37]\n",
            "Validation Epoch 5: 100%|██████████| 6/6 [00:27<00:00,  4.65s/it, loss=1.28]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Training Loss: 1.3686\n",
            "Training Precision: 0.54, Recall: 0.44, F1: 0.43\n",
            "Validation Loss: 1.2803\n",
            "Validation Precision: 0.39, Recall: 0.55, F1: 0.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 21/21 [04:35<00:00, 13.14s/it, loss=1.17]\n",
            "Validation Epoch 6: 100%|██████████| 6/6 [00:28<00:00,  4.67s/it, loss=1.02]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Training Loss: 1.1743\n",
            "Training Precision: 0.70, Recall: 0.55, F1: 0.57\n",
            "Validation Loss: 1.0151\n",
            "Validation Precision: 0.51, Recall: 0.63, F1: 0.56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 21/21 [04:39<00:00, 13.30s/it, loss=0.919]\n",
            "Validation Epoch 7: 100%|██████████| 6/6 [00:28<00:00,  4.73s/it, loss=0.802]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Training Loss: 0.9187\n",
            "Training Precision: 0.76, Recall: 0.65, F1: 0.64\n",
            "Validation Loss: 0.8020\n",
            "Validation Precision: 0.80, Recall: 0.68, F1: 0.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 21/21 [04:34<00:00, 13.08s/it, loss=0.769]\n",
            "Validation Epoch 8: 100%|██████████| 6/6 [00:28<00:00,  4.74s/it, loss=0.633]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Training Loss: 0.7692\n",
            "Training Precision: 0.78, Recall: 0.76, F1: 0.76\n",
            "Validation Loss: 0.6334\n",
            "Validation Precision: 0.77, Recall: 0.75, F1: 0.73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 21/21 [04:34<00:00, 13.07s/it, loss=0.567]\n",
            "Validation Epoch 9: 100%|██████████| 6/6 [00:28<00:00,  4.68s/it, loss=0.545]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Training Loss: 0.5673\n",
            "Training Precision: 0.86, Recall: 0.82, F1: 0.83\n",
            "Validation Loss: 0.5448\n",
            "Validation Precision: 0.94, Recall: 0.77, F1: 0.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 21/21 [04:34<00:00, 13.09s/it, loss=0.522]\n",
            "Validation Epoch 10: 100%|██████████| 6/6 [00:27<00:00,  4.63s/it, loss=0.677]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Training Loss: 0.5219\n",
            "Training Precision: 0.87, Recall: 0.83, F1: 0.85\n",
            "Validation Loss: 0.6772\n",
            "Validation Precision: 0.82, Recall: 0.73, F1: 0.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hwcD-kt4mBWD",
        "PwbLeozn_aJu",
        "ESW3h_B41bYk",
        "voSiefw5H4dn",
        "_5SJZgsy4E-R"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyOoJN0AvDvIs8y19DB5t3JX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}